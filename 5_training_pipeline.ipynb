{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "This notebook will serve as a continuation and advancement of the steps and implementations covered in the gradient descent notebook. We go over implementing loss and optimization via Pytorch, as well as an actual model. We also see the steps involved in setting up the \"pipeline\" for the process. <br>\n",
    "\n",
    "The steps involved are:\n",
    "1. Design the model (input and output size, forward pass)\n",
    "2. Construct the loss and optimizer\n",
    "3. Training loop\n",
    "    1. Forward pass: compute prediction\n",
    "    2. Backward pass: gradients\n",
    "    3. Update weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn  # Neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(17) = 0.000\n",
      "Epoch 1: weight = 0.300, loss = 30.00000000\n",
      "Epoch 11: weight = 1.665, loss = 1.16278565\n",
      "Epoch 21: weight = 1.934, loss = 0.04506890\n",
      "Epoch 31: weight = 1.987, loss = 0.00174685\n",
      "Epoch 41: weight = 1.997, loss = 0.00006770\n",
      "Epoch 51: weight = 1.999, loss = 0.00000262\n",
      "Epoch 61: weight = 2.000, loss = 0.00000010\n",
      "Epoch 71: weight = 2.000, loss = 0.00000000\n",
      "Epoch 81: weight = 2.000, loss = 0.00000000\n",
      "Epoch 91: weight = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(17) = 34.000\n"
     ]
    }
   ],
   "source": [
    "# The same example as before, completely in Torch\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)  # 1, 2, 3, 4 are training samples\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)  # 2, 4, 6, 8 are corresponding outputs\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  # initial weight\n",
    "\n",
    "# model prediction\n",
    "def forward_pass(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(17) = {forward_pass(17):.3f}')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100  # Numerical gradient computation is more accurate\n",
    "# Backpropagation less so, but more suited for complex tasks\n",
    "# We'll just take more epochs for this\n",
    "\n",
    "# Pytorch loss and optimizer\n",
    "loss = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)  # Stochastic Gradient Descent\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward_pass(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradients\n",
    "    l.backward()  # Pytorch's backward pass dl/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Additionally reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: weight = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(17) = {forward_pass(17):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(17) = -9.033\n",
      "Epoch 1: weight = -0.175, loss = 45.67658234\n",
      "Epoch 21: weight = 1.638, loss = 0.17393965\n",
      "Epoch 41: weight = 1.703, loss = 0.12727781\n",
      "Epoch 61: weight = 1.721, loss = 0.11287437\n",
      "Epoch 81: weight = 1.737, loss = 0.10011701\n",
      "Epoch 101: weight = 1.753, loss = 0.08880156\n",
      "Epoch 121: weight = 1.767, loss = 0.07876502\n",
      "Epoch 141: weight = 1.781, loss = 0.06986274\n",
      "Epoch 161: weight = 1.793, loss = 0.06196666\n",
      "Epoch 181: weight = 1.805, loss = 0.05496305\n",
      "Prediction after training: f(17) = 31.416\n"
     ]
    }
   ],
   "source": [
    "# We now replace the forward pass method with a Pytorch module\n",
    "# We also don't need to define weights because the model has params\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # 1, 2, 3, 4 are training samples\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)  # 2, 4, 6, 8 are corresponding outputs\n",
    "# Note the difference in shape, this now a 2D tensor\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)  # 4 samples and 1 feature for each sample\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = nn.Linear(input_size, output_size)  # Linear model with one input and one output\n",
    "\n",
    "test_val = torch.tensor([17], dtype=torch.float32)\n",
    "\n",
    "print(f'Prediction before training: f(17) = {model(test_val).item():.3f}')\n",
    "# gotta call .item() to get access to the actual value instead of the tensor\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "# Backpropagation less so, but more suited for complex tasks\n",
    "# We'll just take more epochs for this\n",
    "\n",
    "# Pytorch loss and optimizer\n",
    "loss = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # Stochastic Gradient Descent\n",
    "# We don't have weights anymore so model's params work\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradients\n",
    "    l.backward()  # Pytorch's backward pass dl/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Additionally reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        # Weights and optional bias have to be unpacked\n",
    "        [w, b] = model.parameters()  # w is a list of lists\n",
    "        print(f'Epoch {epoch + 1}: weight = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(17) = {model(test_val).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above, we used a simple in-built model because the task at hand is simple\n",
    "# We usually end up defining our own models as a class\n",
    "# Here is an example of creating a model for the above task\n",
    "\n",
    "class LinearRegression(nn.Module):  # Inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim):  # Define params as needed\n",
    "        super(LinearRegression, self).__init__() # Call super class constructor\n",
    "        # Define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# THIS IS AN EXTREMELY SIMPLE EXAMPLE, WE WILL SEE MORE COMPLEX ONES LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(17) = 9.476\n",
      "Epoch 1: weight = 0.798, loss = 17.06891251\n",
      "Epoch 21: weight = 1.900, loss = 0.01867640\n",
      "Epoch 41: weight = 1.933, loss = 0.00644375\n",
      "Epoch 61: weight = 1.937, loss = 0.00570869\n",
      "Epoch 81: weight = 1.941, loss = 0.00506346\n",
      "Epoch 101: weight = 1.944, loss = 0.00449118\n",
      "Epoch 121: weight = 1.948, loss = 0.00398359\n",
      "Epoch 141: weight = 1.951, loss = 0.00353335\n",
      "Epoch 161: weight = 1.954, loss = 0.00313399\n",
      "Epoch 181: weight = 1.956, loss = 0.00277978\n",
      "Prediction after training: f(17) = 33.419\n"
     ]
    }
   ],
   "source": [
    "# We'll repeat the above example using this custom model\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # 1, 2, 3, 4 are training samples\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)  # 2, 4, 6, 8 are corresponding outputs\n",
    "# Note the difference in shape, this now a 2D tensor\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)  # 4 samples and 1 feature for each sample\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "test_val = torch.tensor([17], dtype=torch.float32)\n",
    "\n",
    "print(f'Prediction before training: f(17) = {model(test_val).item():.3f}')\n",
    "# gotta call .item() to get access to the actual value instead of the tensor\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "# Backpropagation less so, but more suited for complex tasks\n",
    "# We'll just take more epochs for this\n",
    "\n",
    "# Pytorch loss and optimizer\n",
    "loss = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # Stochastic Gradient Descent\n",
    "# We don't have weights anymore so model's params work\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradients\n",
    "    l.backward()  # Pytorch's backward pass dl/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Additionally reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        # Weights and optional bias have to be unpacked\n",
    "        [w, b] = model.parameters()  # w is a list of lists\n",
    "        print(f'Epoch {epoch + 1}: weight = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(17) = {model(test_val).item():.3f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
