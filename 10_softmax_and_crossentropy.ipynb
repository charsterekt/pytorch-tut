{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation and CrossEntropy loss\n",
    "\n",
    "In this notebook we're going to go over Softmax as an activation function (which returns probability) and CrossEntropy as a loss function. <br>\n",
    "\n",
    "Softmax is an activation function that is commonly used for classification tasks, it is applied before the output layer to squash the incoming values between 0 and 1 to represent a list of probabilities, and the highest probability is taken as the answer for classification. <br>\n",
    "\n",
    "CrossEntropy loss measures the performance of a classification model whose output is a probability between 0 and 1, and can be used in multi class problems. The loss increases as the predicted probability diverges from the actual label, so the better the prediction, the lower the loss will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax numpy:  [0.65900114 0.24243297 0.09856589]\n",
      "Softmax torch:  tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "# A numpy implementation of softmax\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print(\"Softmax numpy: \", outputs)\n",
    "\n",
    "# Torch version\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)  # Computes along the first axis\n",
    "print(\"Softmax torch: \", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy numpy good: 0.3567\n",
      "Cross entropy numpy bad:  2.3026\n",
      "Cross entropy torch good: 0.3018\n",
      "Cross entropy torch bad:  1.6242\n",
      "Predictions good: tensor([2, 0, 1])\n",
      "Predictions bad:  tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# A numpy implementation of cross entropy\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss  # / float(predicted.shape[0])\n",
    "    # The commented out part would be used to normalize it\n",
    "\n",
    "# The input to cross entropy must be a one hot encoded vector\n",
    "y = np.array([1, 0, 0])  # Class 1, 2, 3 represented as True/False by 1/0\n",
    "y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(y, y_pred_good)\n",
    "l2 = cross_entropy(y, y_pred_bad)\n",
    "print(f\"Cross entropy numpy good: {l1:.4f}\")\n",
    "print(f\"Cross entropy numpy bad:  {l2:.4f}\")\n",
    "\n",
    "# Torch version\n",
    "# The loss function in torch already implements softmax so don't implement it in the model\n",
    "# Additionally there is no need to one-hot encode the labels, torch will do it\n",
    "# Leave the class labels as they are\n",
    "# The predictions will have raw scores, no softmax\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "\n",
    "# 3 samples\n",
    "y = torch.tensor([2, 0, 1])  # Class labels, not one hot encoded\n",
    "# Size: n_samples x n_classes = 3 x 3\n",
    "# Raw values, no softmax\n",
    "y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]]) \n",
    "y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(y_pred_good, y)\n",
    "l2 = loss(y_pred_bad, y)\n",
    "\n",
    "print(f\"Cross entropy torch good: {l1.item():.4f}\")\n",
    "print(f\"Cross entropy torch bad:  {l2.item():.4f}\")\n",
    "\n",
    "# Let's get the actual predictions\n",
    "_, predictions1 = torch.max(y_pred_good, 1)  # along the first dimension\n",
    "_, predictions2 = torch.max(y_pred_bad, 1)\n",
    "\n",
    "print(f\"Predictions good: {predictions1}\")\n",
    "print(f\"Predictions bad:  {predictions2}\")\n",
    "\n",
    "# NOTE: the loss in pytorch allows for multiple samples as demonstrated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how a model might be implemented for multiclass problems\n",
    "# Keeping in mind not to use softmax in the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)  # output size 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # NO SOFTMAX\n",
    "        return out\n",
    "        # FOR BINARY CLASSIFICATION\n",
    "        # y_pred = torch.sigmoid(out)\n",
    "        # return y_pred\n",
    "\n",
    "model = NeuralNet(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "loss = nn.CrossEntropyLoss()  # Applies softmax\n",
    "# loss = nn.BCELoss()  # for binary classification\n",
    "# We would use sigmoid for single output and use binary cross entropy loss\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
