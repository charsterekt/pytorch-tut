{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "Autograd is a package within Pytorch and is used to calculate gradients. Gradients are essential for model optimization. Autograd handles all the computation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9632,  0.7163, -0.7456], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)  # Random 1D tensor\n",
    "print(x)  # Remember the requires_grad arg from tensors?\n",
    "# Pytorch will maintain a computational graph for us.\n",
    "# randn and rand are functionally similar\n",
    "# randn draws values from a standard normal distribution (mean is 0, variance is 1)\n",
    "# rand draws values from a uniform distribution i.e. range is [0, 1)\n",
    "# Both return random tensors based on the values from the distributions, and dimensions passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0368, 2.7163, 1.2544], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2  # This will calculate the gradient of y wrt x after the operation\n",
    "y  # Notice the grad_fn attribute, which shows the function that calculated the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1500, 14.7566,  3.1471], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6846, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3824, 3.6217, 1.6726])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And if we now want to calc the gradient:\n",
    "z.backward()  # will calculate the gradient as dz/dx\n",
    "# This will be stored in the grad attribute of x where gradients are stored\n",
    "x.grad  # Shows the final calculated gradient\n",
    "# All of the above only works if requires_grad is specified to be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `grad_fn` usually has a \"Backward\" in it. This is because the gradient is being calculated via BackPropagation for each operation performed. In the first case, `dy/dx` gradient is calculated for the operation `x + 2` in the function `AddBackward`, similarly `y * y * 2` for `dz/dy` in `MulBackward`, and so finally when we call `backward()` on `z`, all the grad functions combine and simplify to calculate the overall gradient for `x` in the form of `dz/dx`. We will discuss backpropagation more in detail in the next notebook. The gradient is calculated using a Jacobian vector matrix of partial derivatives which we multiply with a gradient vector to get the final gradients we want. This is also called the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example z was a scalar so we didn't have to put any arguments in backward(). If we didn't call mean() on z, we would have to create a vector of the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9073,  1.7299,  0.0849], requires_grad=True)\n",
      "tensor([1.0927, 3.7299, 2.0849], grad_fn=<AddBackward0>)\n",
      "tensor([ 2.3881, 27.8247,  8.6933], grad_fn=<MulBackward0>)\n",
      "tensor([4.3709e-01, 1.4920e+01, 8.3394e-03])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 2\n",
    "print(z)\n",
    "# z = z.mean()\n",
    "# We are not calling mean\n",
    "# We create a vector of the same dimensions as z\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "# Pass the vector to backward()\n",
    "z.backward(v)  # Error without vector\n",
    "print(x.grad)\n",
    "# In the background this is a vector Jacobian product.\n",
    "# If it is not a scalar we MUST give it a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prevent PyTorch from tracking history and calculating `grad_fn`. For example when updating weights during our training loop, this operation should not be a part of it. A complete example will be demonstrated in a later notebook.\n",
    "\n",
    "```py\n",
    "x.requires_grad_(False)  # Note the trailing underscore\n",
    "# Recall trailing underscore implies modification of the value in-place\n",
    "x.detach()  # Creates a new tensor with the same values that doesn't require_grad\n",
    "with torch.no_grad():  # Wrapper\n",
    "    # operations here\n",
    "```\n",
    "These are all valid operations. <br>\n",
    "\n",
    "NOTE: Whenever we call the backward() function the gradient for the tensor will be accumulated in the grad attriute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Dummy training example:\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "# One iteration\n",
    "model_output = (weights * 3).sum()  # Dummy operation\n",
    "model_output.backward()\n",
    "print(weights.grad)\n",
    "# Two iterations:\n",
    "model_output = (weights * 3).sum()  # Dummy operation\n",
    "model_output.backward()\n",
    "print(weights.grad)\n",
    "# Three iterations:\n",
    "model_output = (weights * 3).sum()  # Dummy operation\n",
    "model_output.backward()\n",
    "print(weights.grad)\n",
    "\n",
    "# We can see the gradients are accumulated but incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Before the optimization and iteration, we have an additional step to call to correct the gradients\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum()  # Dummy operation\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.data.zero_()  # IMPORTANT: This is the step to correct the gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with optimizers during training steps, we will follow a similar process. This will be covered in a later notebook, but we will go over a short example here:\n",
    "\n",
    "```py\n",
    "optimizer = torch.optim.AdamW(weights, lr=0.01)  # lr = learning rate\n",
    "# There are many optimizers like SGD (stochastic gradient descent)\n",
    "# But Adam is widely used\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
