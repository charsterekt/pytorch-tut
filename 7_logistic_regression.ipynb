{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We have spent the last couple of notebooks already implementing linear regression, we will follow the same training pipeline steps and implement logistic regression now. It should be quite similar to the previous example, with only slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets  # Binary classification dataset\n",
    "from sklearn.preprocessing import StandardScaler  # Scale features\n",
    "from sklearn.model_selection import train_test_split  # Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "Epoch 50: Loss = 0.3225\n",
      "Epoch 100: Loss = 0.2403\n",
      "Epoch 150: Loss = 0.2001\n",
      "Epoch 200: Loss = 0.1753\n",
      "Epoch 250: Loss = 0.1580\n",
      "Epoch 300: Loss = 0.1453\n",
      "Epoch 350: Loss = 0.1354\n",
      "Epoch 400: Loss = 0.1275\n",
      "Epoch 450: Loss = 0.1209\n",
      "Epoch 500: Loss = 0.1155\n",
      "Accuracy: 92.9825%\n"
     ]
    }
   ],
   "source": [
    "# DATASET GENERATION\n",
    "# Binary classification problem based on the popular breast cancer dataset\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target  # Features and labels\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=1234)\n",
    "# 20% of the data will be used for testing\n",
    "\n",
    "# Scale the features to have 0 mean and unit variance\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "# convert to torch tensors\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# reshape y_train and y_test to be 2D tensors\n",
    "y_train = y_train.view(y_train.shape[0], 1)  # Make it a column vector\n",
    "y_test = y_test.view(y_test.shape[0], 1)  # Make it a column vector\n",
    "\n",
    "\n",
    "# MODEL SETUP\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # One output only as it is binary classification\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sigmoid activation function\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "\n",
    "# LOSS AND OPTIMIZER\n",
    "learning_rate = 0.01\n",
    "loss = nn.BCELoss()  # Binary cross entropy loss\n",
    "# Stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TRAINING LOOP\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_predicted = model(x_train)\n",
    "    # loss calculation\n",
    "    loss_value = loss(y_predicted, y_train)\n",
    "    # backward pass\n",
    "    loss_value.backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss = {loss_value.item():.4f}')\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# This shouldn't be part of the computational graph\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(x_test)\n",
    "    # Predicted classes\n",
    "    # Since sigmoid returns values b/w 0 and 1\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "\n",
    "    # divide by number of test samples\n",
    "    accuracy = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'Accuracy: {accuracy * 100:.4f}%')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
