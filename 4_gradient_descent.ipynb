{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent using Autograd\n",
    "\n",
    "We will see how to optimize a model using automatic gradient computation using Pytorch's autograd package. We will see how to implement linear regression from scratch, implement the equations for model prediction and loss function, numerical computation of the gradients and implement the formulae, and implement gradient descent for parameter optimization. We can then see how to replace manually calculated gradients using autograd, and replace the manual loss and optimizer steps, as well as the model with Pytorch's implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(17) = 0.000\n",
      "Epoch 1: weight = 1.200, loss = 30.00000000\n",
      "Epoch 3: weight = 1.872, loss = 0.76800019\n",
      "Epoch 5: weight = 1.980, loss = 0.01966083\n",
      "Epoch 7: weight = 1.997, loss = 0.00050331\n",
      "Epoch 9: weight = 1.999, loss = 0.00001288\n",
      "Epoch 11: weight = 2.000, loss = 0.00000033\n",
      "Epoch 13: weight = 2.000, loss = 0.00000001\n",
      "Epoch 15: weight = 2.000, loss = 0.00000000\n",
      "Epoch 17: weight = 2.000, loss = 0.00000000\n",
      "Epoch 19: weight = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(17) = 34.000\n"
     ]
    }
   ],
   "source": [
    "# MANUAL IMPLEMENTATION\n",
    "# f = w * x (a linear combination of weights and inputs as any function)\n",
    "# Let's go with f = 2 * x, so weight is 2. Ignore bias.\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)  # 1, 2, 3, 4 are training samples\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)  # 2, 4, 6, 8 are corresponding outputs\n",
    "\n",
    "w = 0.0  # initial weight\n",
    "\n",
    "# model prediction\n",
    "def forward_pass(x):\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss_function(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean() # mean squared error\n",
    "\n",
    "# gradient\n",
    "# mean squared error = 1/N * (w * x - y) ** 2\n",
    "# dJ/dw = 1/N 2 * (w * x - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2 * x, (y_predicted - y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(17) = {forward_pass(17):.3f}')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward_pass(X)\n",
    "    # loss\n",
    "    l = loss_function(Y, y_pred)\n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    # We go in the negative direction of training of gradient\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'Epoch {epoch + 1}: weight = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(17) = {forward_pass(17):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(17) = 0.000\n",
      "Epoch 1: weight = 0.300, loss = 30.00000000\n",
      "Epoch 11: weight = 1.665, loss = 1.16278565\n",
      "Epoch 21: weight = 1.934, loss = 0.04506890\n",
      "Epoch 31: weight = 1.987, loss = 0.00174685\n",
      "Epoch 41: weight = 1.997, loss = 0.00006770\n",
      "Epoch 51: weight = 1.999, loss = 0.00000262\n",
      "Epoch 61: weight = 2.000, loss = 0.00000010\n",
      "Epoch 71: weight = 2.000, loss = 0.00000000\n",
      "Epoch 81: weight = 2.000, loss = 0.00000000\n",
      "Epoch 91: weight = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(17) = 34.000\n"
     ]
    }
   ],
   "source": [
    "# TORCH IMPLEMENTATION\n",
    "# We still do loss manually without optimization\n",
    "# f = w * x (a linear combination of weights and inputs as any function)\n",
    "# Let's go with f = 2 * x, so weight is 2. Ignore bias.\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)  # 1, 2, 3, 4 are training samples\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)  # 2, 4, 6, 8 are corresponding outputs\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  # initial weight\n",
    "\n",
    "# model prediction\n",
    "def forward_pass(x):\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss_function(y, y_predicted):\n",
    "    return ((y_predicted - y) ** 2).mean() # mean squared error\n",
    "\n",
    "print(f'Prediction before training: f(17) = {forward_pass(17):.3f}')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100  # Numerical gradient computation is more accurate\n",
    "# Backpropagation less so, but more suited for complex tasks\n",
    "# We'll just take more epochs for this\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward_pass(X)\n",
    "    # loss\n",
    "    l = loss_function(Y, y_pred)\n",
    "    # gradients\n",
    "    l.backward()  # Pytorch's backward pass dl/dw\n",
    "\n",
    "    # update weights\n",
    "    # We go in the negative direction of training of gradient\n",
    "    # In this case it should not be part of the gradient computation graph\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # Additionally reset gradients to zero\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: weight = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(17) = {forward_pass(17):.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce0e62306dd6a5716965d4519ada776f947e6dfc145b604b11307c10277ef29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
